{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename):\n",
    "    with open(filename) as infile:\n",
    "        content = infile.read()\n",
    "    \n",
    "    model = re.findall(r'Model\\s*:\\s*(.*)', content)\n",
    "    wer = re.findall(r'WER\\s*:\\s*(.*)', content)\n",
    "    rtf = re.findall(r'RTF\\s*:\\s*(.*)', content)\n",
    "    memory = re.findall(r'GPU memory while loaded \\[MB\\]\\s*:\\s*(.*)', content)\n",
    "\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'wer': wer,\n",
    "        'rtf': rtf,\n",
    "        'memory': memory\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_data('whisper_results.txt')\n",
    "models = data['model'].unique()\n",
    "\n",
    "tiny_flash = data[data['model'] == models[0]].reset_index(drop=True)\n",
    "base_flash = data[data['model'] == models[1]].reset_index(drop=True)\n",
    "small_flash = data[data['model'] == models[2]].reset_index(drop=True)\n",
    "medium_flash = data[data['model'] == models[3]].reset_index(drop=True)\n",
    "large_flash = data[data['model'] == models[4]].reset_index(drop=True)\n",
    "tiny = data[data['model'] == models[5]].reset_index(drop=True)\n",
    "base = data[data['model'] == models[6]].reset_index(drop=True)\n",
    "small = data[data['model'] == models[7]].reset_index(drop=True)\n",
    "medium = data[data['model'] == models[8]].reset_index(drop=True)\n",
    "large = data[data['model'] == models[9]].reset_index(drop=True)\n",
    "\n",
    "models = [tiny_flash, base_flash, small_flash, medium_flash, large_flash, tiny, base, small, medium, large]\n",
    "\n",
    "for model in models:\n",
    "    model.index = model.index + 1\n",
    "\n",
    "# rename all models by removing openai/whisper- prefix\n",
    "for model in models:\n",
    "    model['model'] = model['model'].str.replace('openai/whisper-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "audio_files = os.listdir(os.path.join(home_dir, 'audio_files'))\n",
    "audio_files.sort()\n",
    "\n",
    "# compute the duration of all audio files\n",
    "\n",
    "durations = []\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    durations.append(sf.info(os.path.join(home_dir, 'audio_files', audio_file)).duration)\n",
    "\n",
    "for i in range(len(models)):\n",
    "    models[i]['duration'] = durations\n",
    "\n",
    "# create a new column inference_time which is the product of the duration and the rtf\n",
    "\n",
    "for i in range(len(models)):\n",
    "    models[i]['inference_time'] = models[i]['duration'] * models[i]['rtf'].astype(float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[346.8157596371882,\n",
       " 212.49412698412698,\n",
       " 188.93167800453514,\n",
       " 395.1161678004535,\n",
       " 276.5463718820862,\n",
       " 123.31208616780046,\n",
       " 823.8900453514739,\n",
       " 321.52922902494333,\n",
       " 154.13657596371883,\n",
       " 246.92351473922903]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot wer\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot the first five subplots\n",
    "for i in range(5):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    models[i]['wer'].astype(float).plot(ax=axs[row, col], label=models[i]['model'][1])\n",
    "    models[i+len(models)//2]['wer'].astype(float).plot(ax=axs[row, col], label=models[i+len(models)//2]['model'][1])\n",
    "    axs[row, col].grid()\n",
    "    axs[row, col].legend()\n",
    "\n",
    "# Remove the last subplot\n",
    "fig.delaxes(axs[1, 2])\n",
    "\n",
    "# Remove titles over the subplots\n",
    "for ax in axs.flat:\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('Audio sample')\n",
    "    ax.set_ylabel('WER [-]')\n",
    "\n",
    "# Create one big title for the full figure\n",
    "fig.suptitle('Comparison of WER between regular and flash attention models')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# export plot to pdf\n",
    "fig.savefig('wer.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rtf\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot the first five subplots\n",
    "for i in range(5):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    models[i]['rtf'].astype(float).plot(ax=axs[row, col], label=models[i]['model'][1])\n",
    "    models[i+len(models)//2]['rtf'].astype(float).plot(ax=axs[row, col], label=models[i+len(models)//2]['model'][1])\n",
    "    axs[row, col].grid()\n",
    "    axs[row, col].legend()\n",
    "\n",
    "# Remove the last subplot\n",
    "fig.delaxes(axs[1, 2])\n",
    "\n",
    "# Remove titles over the subplots\n",
    "for ax in axs.flat:\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('Audio sample')\n",
    "    ax.set_ylabel('RTF [-]')\n",
    "\n",
    "# Create one big title for the full figure\n",
    "fig.suptitle('Comparison of RTF between regular and flash attention models')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# export plot to pdf\n",
    "fig.savefig('rtf.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot memory\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Define the width of the group\n",
    "width = 0.3\n",
    "\n",
    "# Separate models into two groups\n",
    "models_regular = [model for model in models if 'flash' not in model['model'][1]]\n",
    "models_flash = [model for model in models if 'flash' in model['model'][1]]\n",
    "\n",
    "# Plot regular models\n",
    "for i, model in enumerate(models_regular):\n",
    "    ax.bar(i - width/2, model['memory'].astype(float), width, color='tab:orange')\n",
    "\n",
    "# Plot flash models\n",
    "for i, model in enumerate(models_flash):\n",
    "    ax.bar(i + width/2, model['memory'].astype(float), width, color='tab:blue')\n",
    "\n",
    "# Set the x-ticks to be the middle of the groups\n",
    "ax.set_xticks(range(len(models)//2))\n",
    "ax.set_xticklabels([model['model'][1] for model in models_regular])\n",
    "ax.set_xlabel('Model size')\n",
    "ax.set_ylabel('Allocated memory [MB]')\n",
    "ax.set_title('Allocated GPU memory for regular and flash attention models')\n",
    "\n",
    "# Create a custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [Line2D([0], [0], color='tab:orange', lw=4),\n",
    "                Line2D([0], [0], color='tab:blue', lw=4)]\n",
    "ax.legend(custom_lines, ['Regular', 'Flash attention'])\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# export plot to pdf\n",
    "fig.savefig('memory.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Define the width of the group\n",
    "width = 0.3\n",
    "\n",
    "# Separate models into two groups\n",
    "models_regular = [model for model in models if 'flash' not in model['model'][1]]\n",
    "models_flash = [model for model in models if 'flash' in model['model'][1]]\n",
    "\n",
    "# Define metrics\n",
    "metrics = ['wer', 'rtf', 'memory', 'inference_time']\n",
    "titles = ['Average WER for regular and flash attention models', 'Average RTF for regular and flash attention models', 'Average allocated GPU memory for regular and flash attention models', 'Average inference time for regular and flash attention models']\n",
    "ylabels = ['WER [-]', 'RTF [-]', 'Allocated memory [MB]', 'Inference time [s]']\n",
    "\n",
    "# Plot each metric\n",
    "for i, (ax, metric, title, ylabel) in enumerate(zip(axs.flat, metrics, titles, ylabels)):\n",
    "    # Plot regular models\n",
    "    for j, model in enumerate(models_regular):\n",
    "        ax.bar(j - width/2, model[metric].astype(float).mean(), width, color='tab:orange')\n",
    "\n",
    "    # Plot flash models\n",
    "    for j, model in enumerate(models_flash):\n",
    "        ax.bar(j + width/2, model[metric].astype(float).mean(), width, color='tab:blue')\n",
    "\n",
    "    # Set the x-ticks to be the middle of the groups\n",
    "    ax.set_xticks(range(len(models)//2))\n",
    "    ax.set_xticklabels([model['model'][1].replace('flash', '') for model in models_regular])\n",
    "    ax.set_xlabel('Model size')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.grid()\n",
    "\n",
    "# Create a custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [Line2D([0], [0], color='tab:orange', lw=4),\n",
    "                Line2D([0], [0], color='tab:blue', lw=4)]\n",
    "axs[0, 0].legend(custom_lines, ['Regular', 'Flash attention'])\n",
    "axs[0, 1].legend(custom_lines, ['Regular', 'Flash attention'])\n",
    "axs[1, 0].legend(custom_lines, ['Regular', 'Flash attention'])\n",
    "axs[1, 1].legend(custom_lines, ['Regular', 'Flash attention'])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# export plot to pdf\n",
    "fig.savefig('avg.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the inference time for each model the same way as the memory plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Define the width of the group\n",
    "width = 0.3\n",
    "\n",
    "# Separate models into two groups\n",
    "models_regular = [model for model in models if 'flash' not in model['model'][1]]\n",
    "models_flash = [model for model in models if 'flash' in model['model'][1]]\n",
    "\n",
    "# Plot regular models\n",
    "for i, model in enumerate(models_regular):\n",
    "    ax.bar(i - width/2, model['inference_time'].mean(), width, color='tab:orange')\n",
    "\n",
    "# Plot flash models\n",
    "for i, model in enumerate(models_flash):\n",
    "    ax.bar(i + width/2, model['inference_time'].mean(), width, color='tab:blue')\n",
    "\n",
    "# Set the x-ticks to be the middle of the groups\n",
    "ax.set_xticks(range(len(models)//2))\n",
    "ax.set_xticklabels([model['model'][1] for model in models_regular])\n",
    "ax.set_title('Average inference times for regular and flash attention models')\n",
    "ax.set_xlabel('Model size')\n",
    "ax.set_ylabel('Inference time [s]')\n",
    "ax.grid()\n",
    "\n",
    "# Create a custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [Line2D([0], [0], color='tab:orange', lw=4),\n",
    "                Line2D([0], [0], color='tab:blue', lw=4)]\n",
    "ax.legend(custom_lines, ['Regular', 'Flash attention'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# export plot to pdf\n",
    "fig.savefig('avg_inf_time.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the inference time for each model the same way as the wer plot\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot the first five subplots\n",
    "for i in range(5):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    models[i]['inference_time'].plot(ax=axs[row, col], label=models[i]['model'][1])\n",
    "    models[i+len(models)//2]['inference_time'].plot(ax=axs[row, col], label=models[i+len(models)//2]['model'][1])\n",
    "    axs[row, col].grid()\n",
    "    axs[row, col].legend()\n",
    "\n",
    "# Remove the last subplot\n",
    "fig.delaxes(axs[1, 2])\n",
    "\n",
    "# Remove titles over the subplots\n",
    "for ax in axs.flat:\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('Audio file')\n",
    "    ax.set_ylabel('Inference time [s]')\n",
    "\n",
    "# Create one big title for the full figure\n",
    "fig.suptitle('Comparison of inference times between regular and flash attention models')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# export plot to pdf\n",
    "fig.savefig('inf_time.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
