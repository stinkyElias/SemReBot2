{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models\n",
    "\n",
    "Loading different model sizes. For every loading and inference, compute and store the memory usage, word error rate and real-time factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny = 'openai/whisper-tiny'\n",
    "base = 'openai/whisper-base'\n",
    "small = 'openai/whisper-small'\n",
    "medium = 'openai/whisper-medium'\n",
    "large = 'openai/whisper-large-v3'\n",
    "\n",
    "models = [tiny, base, small, medium, large]\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "audio_files = os.listdir(os.path.join(home_dir, 'audio_files'))\n",
    "\n",
    "audio_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file to store test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = 'whisper_results.txt'\n",
    "if os.path.exists(results_file):\n",
    "    os.remove(results_file)\n",
    "\n",
    "header = \"******** Whisper Automatic Speech Recognition test ********\\n\"\n",
    "underline = \"___________________________________________________________\\n\"\n",
    "\n",
    "with open(results_file, 'a') as f:\n",
    "    f.write(header)\n",
    "    f.write(underline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing pipeline with flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-tiny finished inference on 01.wav\n",
      "Model openai/whisper-tiny finished inference on 02.wav\n",
      "Model openai/whisper-tiny finished inference on 03.wav\n",
      "Model openai/whisper-tiny finished inference on 04.wav\n",
      "Model openai/whisper-tiny finished inference on 05.wav\n",
      "Model openai/whisper-tiny finished inference on 06.wav\n",
      "Model openai/whisper-tiny finished inference on 07.wav\n",
      "Model openai/whisper-tiny finished inference on 08.wav\n",
      "Model openai/whisper-tiny finished inference on 09.wav\n",
      "Model openai/whisper-tiny finished inference on 10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-base finished inference on 01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-base finished inference on 02.wav\n",
      "Model openai/whisper-base finished inference on 03.wav\n",
      "Model openai/whisper-base finished inference on 04.wav\n",
      "Model openai/whisper-base finished inference on 05.wav\n",
      "Model openai/whisper-base finished inference on 06.wav\n",
      "Model openai/whisper-base finished inference on 07.wav\n",
      "Model openai/whisper-base finished inference on 08.wav\n",
      "Model openai/whisper-base finished inference on 09.wav\n",
      "Model openai/whisper-base finished inference on 10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-small finished inference on 01.wav\n",
      "Model openai/whisper-small finished inference on 02.wav\n",
      "Model openai/whisper-small finished inference on 03.wav\n",
      "Model openai/whisper-small finished inference on 04.wav\n",
      "Model openai/whisper-small finished inference on 05.wav\n",
      "Model openai/whisper-small finished inference on 06.wav\n",
      "Model openai/whisper-small finished inference on 07.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-small finished inference on 08.wav\n",
      "Model openai/whisper-small finished inference on 09.wav\n",
      "Model openai/whisper-small finished inference on 10.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed3e560eab74a6f98e0d0eac03971d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77a99e4906d41aca4219005d1c208ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbb0c5fdee44a28b8285fd5e0530666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc82dbd37f74d0cbb33a3bf4d163ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29673375e8874c699f428fc486c6dfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e12738148c466890348d5981ba0c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579106b2047d4ad39b8b18deca41827e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcae9b0d2ccb4d12a8faa1627ce3d4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a480b0e4de146328aad54798d496720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092f5f5324b04e6fa44e9d8be57083c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-medium finished inference on 01.wav\n",
      "Model openai/whisper-medium finished inference on 02.wav\n",
      "Model openai/whisper-medium finished inference on 03.wav\n",
      "Model openai/whisper-medium finished inference on 04.wav\n",
      "Model openai/whisper-medium finished inference on 05.wav\n",
      "Model openai/whisper-medium finished inference on 06.wav\n",
      "Model openai/whisper-medium finished inference on 07.wav\n",
      "Model openai/whisper-medium finished inference on 08.wav\n",
      "Model openai/whisper-medium finished inference on 09.wav\n",
      "Model openai/whisper-medium finished inference on 10.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8144bd37b1474fa0ac3ca2f2a7c48a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86a11ee5ebc48028b25af0327351091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19f487a316342f58e3781944e4bdab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4887b1d9fa4509aec9b40feff5d76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01739edc0e834b8d861ddcebeb89c187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aaf792b441476db34d989018e19da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3107b145a3c47edbfd56f3e2702af55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f147122e38174afe98bb0a76d613d490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28a35f4a36b4f38a05f27cf5355890c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802b1cc6f0b14a1287ecbc3e84deeadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-large-v3 finished inference on 01.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-large-v3 finished inference on 02.wav\n",
      "Model openai/whisper-large-v3 finished inference on 03.wav\n",
      "Model openai/whisper-large-v3 finished inference on 04.wav\n",
      "Model openai/whisper-large-v3 finished inference on 05.wav\n",
      "Model openai/whisper-large-v3 finished inference on 06.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/whisper-large-v3 finished inference on 07.wav\n",
      "Model openai/whisper-large-v3 finished inference on 08.wav\n",
      "Model openai/whisper-large-v3 finished inference on 09.wav\n",
      "Model openai/whisper-large-v3 finished inference on 10.wav\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    torch.cuda.empty_cache()\n",
    "    asr_pipeline = pipeline('automatic-speech-recognition',\n",
    "                            model=model,\n",
    "                            torch_dtype=torch.float16,\n",
    "                            device='cuda:0',\n",
    "                            model_kwargs={'attn_implementation': 'flash_attention_2'} if is_flash_attn_2_available() else {'attn_implementation': 'sdpa'}\n",
    "    )\n",
    "\n",
    "    # check how much memory the model is using when loaded\n",
    "    gpu_mem_loaded = torch.cuda.memory_allocated('cuda:0')\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        \n",
    "        output = asr_pipeline(\n",
    "            os.path.join(home_dir, 'audio_files', audio_file),\n",
    "            chunk_length_s=30,\n",
    "            batch_size=24,\n",
    "            return_timestamps=True\n",
    "        )\n",
    "\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        end.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        timer = start.elapsed_time(end)/1000\n",
    "\n",
    "        gpu_mem_inf = torch.cuda.memory_allocated('cuda:0')\n",
    "        delta_gpu_mem = gpu_mem_inf-gpu_mem_loaded\n",
    "\n",
    "        # extract audio file duration and compute Real-Time Factor\n",
    "        audio_info = sf.info(os.path.join(home_dir, 'audio_files', audio_file))\n",
    "        rtf = timer/audio_info.duration\n",
    "\n",
    "        # compute Word Error Rate\n",
    "        with open(f'references/{audio_file[:-4]}.txt', 'r') as f:\n",
    "            reference = f.read().replace('\\n', ' ')\n",
    "        \n",
    "        # if first char in hypothesis is whitespace, remove it\n",
    "        if output['text'][0] == ' ':\n",
    "            output['text'] = output['text'][1:]\n",
    "        hypothesis = output['text']\n",
    "        wer_result = wer(reference, hypothesis)\n",
    "\n",
    "        result = {\n",
    "                  'Model': model + '-flash',\n",
    "                  'Audio file': audio_file,\n",
    "                  'WER': round(wer_result, 5),\n",
    "                  'RTF': round(rtf, 5),\n",
    "                  'GPU memory while loaded [MB]': round(gpu_mem_loaded*1e-6, 5),\n",
    "                #   'GPU memory while inference [MB]': round(gpu_mem_inf*1e-6, 5),\n",
    "                #   'Delta GPU memory [MB]': round(delta_gpu_mem*1e-6, 5),\n",
    "        }\n",
    "\n",
    "        with open(results_file, 'a') as infile:\n",
    "            for key, value in result.items():\n",
    "                infile.write(f'{key:<32}: {value}\\n')\n",
    "            infile.write(underline)\n",
    "        \n",
    "        print(f'Model {model} finished inference on {audio_file}')\n",
    "    \n",
    "    del asr_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing pipeline without flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper import load_model\n",
    "\n",
    "models = ['tiny', 'base', 'small', 'medium', 'large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tiny finished inference on 01.wav\n",
      "Model tiny finished inference on 02.wav\n",
      "Model tiny finished inference on 03.wav\n",
      "Model tiny finished inference on 04.wav\n",
      "Model tiny finished inference on 05.wav\n",
      "Model tiny finished inference on 06.wav\n",
      "Model tiny finished inference on 07.wav\n",
      "Model tiny finished inference on 08.wav\n",
      "Model tiny finished inference on 09.wav\n",
      "Model tiny finished inference on 10.wav\n",
      "Model base finished inference on 01.wav\n",
      "Model base finished inference on 02.wav\n",
      "Model base finished inference on 03.wav\n",
      "Model base finished inference on 04.wav\n",
      "Model base finished inference on 05.wav\n",
      "Model base finished inference on 06.wav\n",
      "Model base finished inference on 07.wav\n",
      "Model base finished inference on 08.wav\n",
      "Model base finished inference on 09.wav\n",
      "Model base finished inference on 10.wav\n",
      "Model small finished inference on 01.wav\n",
      "Model small finished inference on 02.wav\n",
      "Model small finished inference on 03.wav\n",
      "Model small finished inference on 04.wav\n",
      "Model small finished inference on 05.wav\n",
      "Model small finished inference on 06.wav\n",
      "Model small finished inference on 07.wav\n",
      "Model small finished inference on 08.wav\n",
      "Model small finished inference on 09.wav\n",
      "Model small finished inference on 10.wav\n",
      "Model medium finished inference on 01.wav\n",
      "Model medium finished inference on 02.wav\n",
      "Model medium finished inference on 03.wav\n",
      "Model medium finished inference on 04.wav\n",
      "Model medium finished inference on 05.wav\n",
      "Model medium finished inference on 06.wav\n",
      "Model medium finished inference on 07.wav\n",
      "Model medium finished inference on 08.wav\n",
      "Model medium finished inference on 09.wav\n",
      "Model medium finished inference on 10.wav\n",
      "Model large finished inference on 01.wav\n",
      "Model large finished inference on 02.wav\n",
      "Model large finished inference on 03.wav\n",
      "Model large finished inference on 04.wav\n",
      "Model large finished inference on 05.wav\n",
      "Model large finished inference on 06.wav\n",
      "Model large finished inference on 07.wav\n",
      "Model large finished inference on 08.wav\n",
      "Model large finished inference on 09.wav\n",
      "Model large finished inference on 10.wav\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    torch.cuda.empty_cache()\n",
    "    asr_pipeline = load_model(model).to('cuda:0')\n",
    "\n",
    "    # check how much memory the model is using when loaded\n",
    "    gpu_mem_loaded = torch.cuda.memory_allocated('cuda:0')\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        \n",
    "        output = asr_pipeline.transcribe(os.path.join(home_dir, 'audio_files', audio_file), fp16=True)\n",
    "\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        end.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        timer = start.elapsed_time(end)/1000\n",
    "\n",
    "        gpu_mem_inf = torch.cuda.memory_allocated('cuda:0')\n",
    "        delta_gpu_mem = gpu_mem_inf-gpu_mem_loaded\n",
    "\n",
    "        # extract audio file duration and compute Real-Time Factor\n",
    "        audio_info = sf.info(os.path.join(home_dir, 'audio_files', audio_file))\n",
    "        rtf = timer/audio_info.duration\n",
    "\n",
    "        # compute Word Error Rate\n",
    "        with open(f'references/{audio_file[:-4]}.txt', 'r') as f:\n",
    "            reference = f.read().replace('\\n', ' ')\n",
    "        \n",
    "        # if first char in hypothesis is whitespace, remove it\n",
    "        if output['text'][0] == ' ':\n",
    "            output['text'] = output['text'][1:]\n",
    "        hypothesis = output['text']\n",
    "        wer_result = wer(reference, hypothesis)\n",
    "\n",
    "        result = {\n",
    "                  'Model': model,\n",
    "                  'Audio file': audio_file,\n",
    "                  'WER': round(wer_result, 5),\n",
    "                  'RTF': round(rtf, 5),\n",
    "                  'GPU memory while loaded [MB]': round(gpu_mem_loaded*1e-6, 5),\n",
    "                #   'GPU memory while inference [MB]': round(gpu_mem_inf*1e-6, 5),\n",
    "                #   'Delta GPU memory [MB]': round(delta_gpu_mem*1e-6, 5),\n",
    "        }\n",
    "\n",
    "        with open(results_file, 'a') as infile:\n",
    "            for key, value in result.items():\n",
    "                infile.write(f'{key:<32}: {value}\\n')\n",
    "            infile.write(underline)\n",
    "        \n",
    "        print(f'Model {model} finished inference on {audio_file}')\n",
    "    \n",
    "    del asr_pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
