{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "data_dir = os.path.join(home, 'Documents/semantic-robot/testing/mistral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename):\n",
    "    with open(filename) as f:\n",
    "        content = f.read()\n",
    "\n",
    "    test_sets = re.findall(r'Test set #\\s+:\\s+(.+)', content)\n",
    "    number_of_examples = re.findall(r'Number of examples\\s+:\\s+(.+)', content)\n",
    "    inference_times = re.findall(r'Inference time \\[s\\]\\s+:\\s+(.+)', content)\n",
    "    memory_usage = re.findall(r'GPU memory loaded \\[MB\\]\\s+:\\s+(.+)', content)\n",
    "    input_size = re.findall(r'Model input size \\[MB\\]\\s+:\\s+(.+)', content)\n",
    "    model = re.findall(r'Model\\s+:\\s+(.+)', content)\n",
    "    instance_match_accuracy = re.findall(r'Instance match accuracy\\s+:\\s+(.+)', content)\n",
    "    predicate_match_accuracy = re.findall(r'Predicate match accuracy\\s+:\\s+(.+)', content)\n",
    "    goal_match_accuracy = re.findall(r'Goal match accuracy\\s+:\\s+(.+)', content)\n",
    "    total_match_accuracy = re.findall(r'Total match accuracy\\s+:\\s+(.+)', content)\n",
    "    labeled = re.findall(r'Labeled\\s+:\\s+(.+)', content)\n",
    "\n",
    "    return {\n",
    "        'model': list(map(str, model)),\n",
    "        'test_set': list(map(int, test_sets)),\n",
    "        'num_examples': list(map(int, number_of_examples)),\n",
    "        'instance_match_accuracy': list(map(float, instance_match_accuracy)),\n",
    "        'predicate_match_accuracy': list(map(float, predicate_match_accuracy)),\n",
    "        'goal_match_accuracy': list(map(float, goal_match_accuracy)),\n",
    "        'total_match_accuracy': list(map(float, total_match_accuracy)),\n",
    "        'inference_time': list(map(float, inference_times)),\n",
    "        'memory_usage': list(map(float, memory_usage)),\n",
    "        'input_size': list(map(float, input_size)),\n",
    "        'labeled': list(map(str, labeled))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files of results from 4-bit, 8-bit, half and full precision for both labeled and unlabeled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4bit = extract_data(os.path.join(data_dir, 'mistral_results_4bit.txt'))\n",
    "data_4bit_labeled = extract_data(os.path.join(data_dir, 'mistral_results_4bit_labeled.txt'))\n",
    "\n",
    "data_8bit = extract_data(os.path.join(data_dir, 'mistral_results_8bit.txt'))\n",
    "data_8bit_labeled = extract_data(os.path.join(data_dir, 'mistral_results_8bit_labeled.txt'))\n",
    "\n",
    "data_half_precision = extract_data(os.path.join(data_dir, 'mistral_results_half_precision.txt'))\n",
    "data_half_precision_labeled = extract_data(os.path.join(data_dir, 'mistral_results_half_precision_labeled.txt'))\n",
    "\n",
    "data_full_precision = extract_data(os.path.join(data_dir, 'mistral_results_full_precision.txt'))\n",
    "data_full_precision_labeled = extract_data(os.path.join(data_dir, 'mistral_results_full_precision_labeled.txt'))\n",
    "\n",
    "df_4bit = pd.DataFrame(data_4bit)\n",
    "df_4bit_labeled = pd.DataFrame(data_4bit_labeled)\n",
    "\n",
    "df_8bit = pd.DataFrame(data_8bit)\n",
    "df_8bit_labeled = pd.DataFrame(data_8bit_labeled)\n",
    "\n",
    "df_half_precision = pd.DataFrame(data_half_precision)\n",
    "df_half_precision_labeled = pd.DataFrame(data_half_precision_labeled)\n",
    "\n",
    "df_full_precision = pd.DataFrame(data_full_precision)\n",
    "df_full_precision_labeled = pd.DataFrame(data_full_precision_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-bit unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_matches = np.zeros(max(df_4bit['num_examples'])-1)\n",
    "pred_matches = np.zeros(max(df_4bit['num_examples'])-1)\n",
    "goal_matches = np.zeros(max(df_4bit['num_examples'])-1)\n",
    "tot_matches = np.zeros(max(df_4bit['num_examples'])-1)\n",
    "inf_times = np.zeros(max(df_4bit['num_examples'])-1)\n",
    "mem_usages = np.zeros(max(df_4bit['num_examples'])-1)\n",
    "input_sizes = np.zeros(max(df_4bit['num_examples'])-1)\n",
    "\n",
    "for i in range(2, 11):\n",
    "    idxs = df_4bit.loc[df_4bit['num_examples'] == i]\n",
    "    \n",
    "    instance_match_accuracy = [float(x) for x in idxs['instance_match_accuracy']]\n",
    "    avg_instance_match_accuracy = sum(instance_match_accuracy) / len(instance_match_accuracy)\n",
    "\n",
    "    predicate_match_accuracy = [float(x) for x in idxs['predicate_match_accuracy']]\n",
    "    avg_predicate_match_accuracy = sum(predicate_match_accuracy) / len(predicate_match_accuracy)\n",
    "\n",
    "    goal_match_accuracy = [float(x) for x in idxs['goal_match_accuracy']]\n",
    "    avg_goal_match_accuracy = sum(goal_match_accuracy) / len(goal_match_accuracy)\n",
    "\n",
    "    total_match_accuracy = [float(x) for x in idxs['total_match_accuracy']]\n",
    "    avg_total_match_accuracy = sum(total_match_accuracy) / len(total_match_accuracy)\n",
    "\n",
    "    inference_times = [float(x) for x in idxs['inference_time']]\n",
    "    avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "    memory_usage = [float(x) for x in idxs['memory_usage']]\n",
    "    avg_memory_usage = sum(memory_usage) / len(memory_usage)\n",
    "\n",
    "    input_size = [float(x) for x in idxs['input_size']]\n",
    "    avg_input_size = sum(input_size) / len(input_size)\n",
    "\n",
    "    inst_matches[i-2] = avg_instance_match_accuracy\n",
    "    pred_matches[i-2] = avg_predicate_match_accuracy\n",
    "    goal_matches[i-2] = avg_goal_match_accuracy\n",
    "    tot_matches[i-2] = avg_total_match_accuracy\n",
    "    inf_times[i-2] = avg_inference_time\n",
    "    mem_usages[i-2] = avg_memory_usage\n",
    "    input_sizes[i-2] = avg_input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(2, 10, len(inf_times))\n",
    "plt.plot(x, inf_times)\n",
    "plt.title('Average inference time for different number of examples')\n",
    "plt.xlabel('Number of examples')\n",
    "plt.ylabel('Inference time [s]')\n",
    "\n",
    "min_inf_time = min(inf_times)\n",
    "max_inf_time = max(inf_times)\n",
    "min_inf_time_idx = np.where(inf_times == min_inf_time)\n",
    "max_inf_time_idx = np.where(inf_times == max_inf_time)\n",
    "plt.plot(x[min_inf_time_idx], min_inf_time, 'go', label=f'Min inf. time: {round(min_inf_time, 1)} s')\n",
    "plt.plot(x[max_inf_time_idx], max_inf_time, 'ro', label=f'Max inf. time: {round(max_inf_time, 1)} s')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(2, 10, len(mem_usages))\n",
    "plt.plot(x, mem_usages)\n",
    "plt.title('Average memory usage for different number of examples')\n",
    "plt.xlabel('Number of examples')\n",
    "plt.ylabel('Memory usage [MB]')\n",
    "\n",
    "min_mem_usage = min(mem_usages)\n",
    "max_mem_usage = max(mem_usages)\n",
    "min_mem_usage_idx = np.where(mem_usages == min_mem_usage)\n",
    "max_mem_usage_idx = np.where(mem_usages == max_mem_usage)\n",
    "\n",
    "plt.plot(x[min_mem_usage_idx[0][0]], min_mem_usage, 'go', label=f'Min mem. usage: {round(min_mem_usage, 1)} MB')\n",
    "plt.plot(x[max_mem_usage_idx[0][0]], max_mem_usage, 'ro', label=f'Max mem. usage: {round(max_mem_usage, 1)} MB')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-bit labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-bit unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-bit labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half precision unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half precision labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full precision unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full precision labeled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
