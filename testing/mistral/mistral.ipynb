{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "idun = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "\n",
    "if idun:\n",
    "    os.environ['HF_HOME'] = '/cluster/work/eliashk/models/mistral/4-bit'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test set and store domains, inputs and corresponding outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shot_set_simple_domain.jsonl', 'r') as infile:\n",
    "    shot_data = json.load(infile)\n",
    "\n",
    "domains, inputs, outputs = [], [], []\n",
    "\n",
    "for i in range(len(shot_data['shots'])):\n",
    "    domains.append(shot_data['shots'][i]['domain'])\n",
    "    inputs.append(shot_data['shots'][i]['input'])\n",
    "    outputs.append(shot_data['shots'][i]['output'])\n",
    "\n",
    "with open('test_set.jsonl', 'r') as infile:\n",
    "    test_data = json.load(infile)\n",
    "\n",
    "test_domains, commands, solutions, num_instances, num_predicates, num_goals = [], [], [], [], [], []\n",
    "\n",
    "for i in range(len(test_data['tests'])):\n",
    "    test_domains.append(test_data['tests'][i]['domain'])\n",
    "    commands.append(test_data['tests'][i]['command'])\n",
    "    solutions.append(test_data['tests'][i]['solution'])\n",
    "    num_instances.append(test_data['tests'][i]['num_instances'])\n",
    "    num_predicates.append(test_data['tests'][i]['num_predicates'])\n",
    "    num_goals.append(test_data['tests'][i]['num_goals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'You are a helpful PDDL assistant that will list up the available instances, predicates and goals for the given domain and natural language command. You can only answer in the desired format.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create file for storing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = 'mistral_results.txt'\n",
    "if os.path.exists(results_file):\n",
    "    os.remove(results_file)\n",
    "\n",
    "device_name = torch.cuda.get_device_properties(device).name\n",
    "device_capacity = round(torch.cuda.get_device_properties(device).total_memory/(1024**2), 0)\n",
    "\n",
    "header =      \"                      Mistral Large Language Model test                    \\n\"\n",
    "underline =   \"===========================================================================\\n\"\n",
    "device_info =f\"  Test completed on device {device_name} with {device_capacity} MB memory  \\n\" \n",
    "\n",
    "\n",
    "with open(results_file, 'a') as outfile:\n",
    "    outfile.write(header)\n",
    "    outfile.write(underline)\n",
    "    outfile.write(device_info)\n",
    "    outfile.write(underline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral 7B 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System messages and lists for storing interesting test data\n",
    "\n",
    "This test data is without prompt classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {'role': 'user', 'content': system_prompt},\n",
    "        {'role': 'assistant', 'content': 'Please provide the domain.pddl and corresponding command.'},\n",
    "        {'role': 'user', 'content': f'domain.pddl: {domains[0]}, command: {inputs[0]}'},\n",
    "        {'role': 'assistant', 'content': 'Understood. What is the expected output format?'},\n",
    "        {'role': 'user', 'content': f'### Expected output ### {outputs[0]}'},\n",
    "    ]\n",
    "\n",
    "allocated, model_input_size, inference_times, f1_scores, semantic_similarities, total_match_accuracies, model_outputs = [], [], [], [], [], [], []\n",
    "instance_match_accuracies, predicate_match_accuracies, goal_match_accuracies = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for computing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(expected, generated):\n",
    "    if expected == None:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return round(generated/expected, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(shot_data['shots'])):\n",
    "# # for i in range(8, 10):\n",
    "    model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n",
    "    memory_allocated_loaded = round(torch.cuda.memory_allocated(device)/(1024**2), 5)               # MB\n",
    "\n",
    "    allocated.append(memory_allocated_loaded)\n",
    "\n",
    "    number_of_examples = i+1\n",
    "    test_set_number = 1\n",
    "    number_of_max_new_tokens = 250  # default\n",
    "\n",
    "    # if the loop has been executed at least once, remove the last two messages which is the ones that have to be last\n",
    "    # the new messages to be added is another example from the shot_data\n",
    "    if i>0:\n",
    "        del messages[-2:]\n",
    "\n",
    "        messages.append({'role': 'assistant', 'content': 'Please provide the domain.pddl and corresponding command.'})\n",
    "        messages.append({'role': 'user', 'content': f'domain.pddl: {domains[i]}, command: {inputs[i]}'})\n",
    "        messages.append({'role': 'assistant', 'content': 'Understood. What is the expected output format?'})\n",
    "        messages.append({'role': 'user', 'content': f'### Expected output ### {outputs[i]}'})\n",
    "\n",
    "    messages.append({'role': 'assistant', 'content': 'Thank you. Ready for the new instruction.'})\n",
    "    messages.append({'role': 'user', 'content': f'domain.pddl: {test_domains[0]}, command: {commands[0]}'})\n",
    "    \n",
    "    # start timer\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "\n",
    "    # inference\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors='pt')\n",
    "    model_inputs = encodeds.to(device)\n",
    "\n",
    "    model_input_size.append(round((model_inputs.element_size()*model_inputs.nelement())/(1024**2), 5))    # MB\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_4bit.generate(\n",
    "            model_inputs,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=number_of_max_new_tokens,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    end.record()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # compute inference time\n",
    "    timer = start.elapsed_time(end)/1000\n",
    "    inference_times.append(timer)\n",
    "\n",
    "    '''\n",
    "    Format the output string - remove the last 4 characters which are the end token,\n",
    "    then remove everything after the last delimiter and\n",
    "    remove everything before the first occurence of \"instance\". Sometimes,\n",
    "    the model outputs \"Expected output:\", so we delete everything before \":\".\n",
    "    Finally, remove newlines in output\n",
    "    '''\n",
    "    output_tokens = decoded[0]\n",
    "    end_token = '[/INST]'\n",
    "\n",
    "    end_tag_index = output_tokens.rfind(end_token)\n",
    "    end_of_sentence = -4\n",
    "    sliced_output = output_tokens[end_tag_index + len(end_token):end_of_sentence]\n",
    "\n",
    "    delimiter = '|'\n",
    "    last_delimiter = sliced_output.rfind(delimiter)\n",
    "    model_output = sliced_output[:last_delimiter+1]\n",
    "\n",
    "    # model_output = model_output[model_output.find('instance'):]\n",
    "    # model_output = ' '.join(model_output.strip().split())\n",
    "\n",
    "    # model_output = model_output[model_output.find(':')+1:]\n",
    "\n",
    "    # remove newlines in output\n",
    "    model_output = model_output.replace('\\n', ' ')\n",
    "    model_output = model_output.replace('\\t', ' ')\n",
    "    model_output = model_output.replace('\\r', ' ')\n",
    "    model_output = model_output.replace('  ', ' ')\n",
    "\n",
    "    model_outputs.append(model_output)\n",
    "\n",
    "    f1_score = None\n",
    "    semantic_similarity = None\n",
    "\n",
    "\n",
    "    # compute the accuracy of amount of instances, predicates and goals in the output\n",
    "    # compared to the solution\n",
    "\n",
    "    # count number of instances, predicates and goals in the model output\n",
    "    num_instances_output = model_output.count('instance')\n",
    "    num_predicates_output = model_output.count('predicate')\n",
    "    num_goals_output = model_output.count('goal')\n",
    "\n",
    "    # compare the number of instances, predicates and goals in the model output to the solution\n",
    "    instance_match_accuracies.append(compute_accuracy(num_instances[0], num_instances_output))\n",
    "    predicate_match_accuracies.append(compute_accuracy(num_predicates[0], num_predicates_output))\n",
    "    goal_match_accuracies.append(compute_accuracy(num_goals[0], num_goals_output))\n",
    "\n",
    "    #compute the average of the three accuracies\n",
    "    total_match_accuracy = round((instance_match_accuracies[-1] + predicate_match_accuracies[-1] + goal_match_accuracies[-1])/3, 2)\n",
    "    total_match_accuracies.append(total_match_accuracy)\n",
    "\n",
    "    f1_scores.append(f1_score)\n",
    "    semantic_similarities.append(semantic_similarity)\n",
    "\n",
    "    # write the results to file\n",
    "    result = {\n",
    "        'Model': model_id + '-4bit',\n",
    "        'Max new tokens': number_of_max_new_tokens,\n",
    "        'Test set #': test_set_number,\n",
    "        'Number of examples': number_of_examples,\n",
    "        'F1Score': f1_scores[-1],\n",
    "        'Semantic similarity': semantic_similarities[-1],\n",
    "        'Instance match accuracy': instance_match_accuracies[-1],\n",
    "        'Predicate match accuracy': predicate_match_accuracies[-1],\n",
    "        'Goal match accuracy': goal_match_accuracies[-1],\n",
    "        'Total match accuracy': total_match_accuracies[-1],\n",
    "        'Inference time [s]': inference_times[-1],\n",
    "        'GPU memory loaded [MB]': allocated[-1],\n",
    "        'Model input size [MB]': model_input_size[-1],\n",
    "        'Sliced output': model_outputs[-1],\n",
    "        'Solution': solutions[0],\n",
    "        'Simple': 'Yes',\n",
    "    }\n",
    "\n",
    "    with open(results_file, 'a') as outfile:\n",
    "        for key, value in result.items():\n",
    "            outfile.write(f'{key:<25}: {value}\\n')\n",
    "        outfile.write(underline)\n",
    "\n",
    "    print(f'Mistral 7B instruct 4-bit finished on test {test_set_number} with {number_of_examples} examples.')\n",
    "\n",
    "    # delete model to free up GPU memory\n",
    "    del model_4bit\n",
    "    del model_inputs\n",
    "    del generated_ids\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # wait for memory to reach <250 MB\n",
    "    while torch.cuda.memory_allocated(device) > 250*(1024**2):\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# clean up\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the allocated and reserverd memory for evey iteration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(1, len(allocated)+1)\n",
    "plt.plot(x, allocated)\n",
    "plt.xlabel(' i-th iteration')\n",
    "plt.ylabel('Memory [MB]')\n",
    "plt.title('Memory allocation after loading model')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x = np.arange(1, len(model_input_size)+1)\n",
    "plt.plot(x, model_input_size)\n",
    "plt.xlabel('i-th iteration')\n",
    "plt.ylabel('Size [MB]')\n",
    "plt.title('Model input size')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# print all lists\n",
    "print(f\"Allocated: {allocated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon",
   "language": "python",
   "name": "falcon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
