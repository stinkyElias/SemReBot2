{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME']='/home/stinky/models'\n",
    "# os.environ['HF_HOME']='/home/carla/elias/models'\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda886b649394a04a98babf535d84e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(:types robot zone pallet ) (:predicates (robot_available ?robot - robot) (robot_at ?robot - robot ?zone - zone) (pallet_at ?pallet - pallet ?zone - zone) (is_pallet ?pallet - pallet) (pallet_not_moved ?pallet - pallet) (is_unload_zone ?zone - zone) (is_shelf_zone ?zone - zone) )'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_pddl_path = 'domain.pddl'\n",
    "\n",
    "with open(domain_pddl_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "domain_without_newlines = data.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "domain = ' '.join(domain_without_newlines.split())\n",
    "\n",
    "# edit domain to only contain the types and predicates\n",
    "domain = domain[domain.find('(:types'):]\n",
    "domain = domain[:domain.rfind(';')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " set instance pallet_1 pallet|\n",
      "\n",
      "set predicate pallet_at pallet_1 unload_zone|\n",
      "set predicate pallet_not_moved pallet_1|\n",
      "\n",
      "set goal pallet_at pallet_1 shelf_2|\n",
      "\n",
      "set instance robot_1 robot|\n",
      "set goal robot_available robot_1|\n",
      "\n",
      "set goal robot_at robot_1 charging\\_station.|\n"
     ]
    }
   ],
   "source": [
    "question = '\"Move the new pallet from the unload zone to shelf 2 and recharge at the charging station.\"'\n",
    "\n",
    "messages = [\n",
    "        {'role': 'user', 'content': 'You will receive a natural language prompt and create text based on the prompt and a domain from domain.pddl.'},\n",
    "        {'role': 'assistant', 'content': 'I understand. Give me the domain.pddl'},\n",
    "        {'role': 'user', 'content': 'domain.pddl: ' + domain},\n",
    "        {'role': 'assistant', 'content': 'Thank you. This is the types and predicates in the domain.pddl.'},\n",
    "        {'role': 'user', 'content': 'Yes. I will show you an example, and you will see the format of a prompt and the only accepted format to answer in. Only answer in the same format as this example.'},\n",
    "        {'role': 'assistant', 'content': 'Understood, I will learn from the coming example and see how I shall respond.'},\n",
<<<<<<< HEAD
    "        {'role': 'user', 'content': 'Prompt: \"A new shipment arrived. Please move the new pallet from the unload zone to reol 1. Afterwards, wait at reol 2\". From this prompt, the only correct answer would be: instance pallet_1 pallet|predicate pallet_at pallet_1 unload_zone|predicate pallet_not_moved pallet_1|goal pallet_at pallet_1 reol_1|goal robot_at tars reol_2|'},\n",
=======
    "        {'role': 'user', 'content': 'Prompt: \"A new shipment arrived. Please move the new pallet from the unload zone to reol 1. Afterwards, wait at reol 2\". From this prompt, the only correct answer would be: set instance pallet_1 pallet|set predicate pallet_at pallet_1 unload_zone|set predicate pallet_not_moved pallet_1|set goal pallet_at pallet_1 reol_1|set goal robot_at tars reol_2|'},\n",
>>>>>>> main
    "        {'role': 'assistant', 'content': 'I understand the format. I will respond in the same format, only setting instances, predicates and goals delimited by \"|\". Please give me the prompt.'},\n",
    "        {'role': 'user', 'content': 'Prompt: ' + question},\n",
    "    ]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors='pt')\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = model_nf4.generate(model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=256, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "# Remove the pre-prompts and end-of-sentence token\n",
    "output_tokens = decoded[0]\n",
    "end_token = \"[/INST]\"\n",
    "\n",
    "# Find last occurence of end_tag in output\n",
    "end_tag_index = output_tokens.rfind(end_token)\n",
    "end_of_sentence = -4\n",
    "sliced_output = output_tokens[end_tag_index + len(end_token):end_of_sentence]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "delimiter = '|'\n",
    "\n",
    "# start from the back of sliced_output and find the last occurence of delimiter\n",
    "last_delimiter = sliced_output.rfind(delimiter)\n",
    "# slice the string from the last occurence of delimiter to the end\n",
    "sliced_output = sliced_output[:last_delimiter + 1]\n",
    "\n",
    "print(sliced_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
