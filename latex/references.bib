
@misc{favorito_ai-planningpddl-generators_2024,
	title = {{AI}-{Planning}/pddl-generators},
	url = {https://github.com/AI-Planning/pddl-generators},
	abstract = {A collection of PDDL generators, some of which have been used to generate benchmarks for the International Planning Competition (IPC).},
	urldate = {2024-04-30},
	publisher = {AI Planning},
	author = {Favorito, Marco and Magnaguagno, Mau},
	month = apr,
	year = {2024},
	note = {original-date: 2020-04-05T17:41:08Z},
}

@misc{revcom_transcript_2024,
	title = {Transcript {Library}: {Transcripts} of {Public} {Speech} - {Rev}},
	shorttitle = {Transcript {Library}},
	url = {https://www.rev.com/blog/transcripts},
	abstract = {Explore audio and video transcripts from political figures, celebrities, entertainers, CEO’s, public figures, and more.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Rev.com},
	year = {2024},
}

@misc{allen_oscars_nodate,
	title = {Oscars {Academy} {Awards} {Jimmy} {Kimmel} {Monologue} {\textbar} {Rev}},
	url = {https://www.rev.com/blog/transcripts/jimmy-kimmels-oscars-monologue-2024},
	abstract = {Jimmy Kimmel's opening monologue from the 96th Academy Awards. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_private_nodate,
	title = {Private {Spacecraft} {Blasts} {Off} to {Attempt} {First} {U}.{S}. {Moon} {Landing} in 52 {Years} {Transcript}},
	url = {https://www.rev.com/blog/transcripts/private-spacecraft-blasts-off-to-attempt-first-u-s-moon-landing-in-52-years-transcript},
	abstract = {A privately-owned spacecraft headed for the Moon has blasted off from Cape Canaveral, Florida. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_leap_nodate,
	title = {Leap {Year} {February} 9th {Leap} {Day} {\textbar} {Rev}},
	url = {https://www.rev.com/blog/transcripts/what-is-a-leap-year-and-why-do-they-happen},
	abstract = {A leap year exists, in large part, to keep the months in sync with annual events, including equinoxes and solstices. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_supreme_nodate,
	title = {Supreme {Court} {Rules} {States} {Cannot} {Remove} {Trump} from {Ballot}},
	url = {https://www.rev.com/blog/transcripts/14th-amendment-supreme-court-ballot-donald-trump-rev},
	abstract = {The U.S. Supreme Court ruled that states cannot remove former president Donald Trump from their ballots based on the 14th Amendment. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_haiti_nodate,
	title = {Haiti {Ariel} {Henry} {Resignation} {Gangs} {Prime} {Minister} {\textbar} {Rev}},
	url = {https://www.rev.com/blog/transcripts/haitian-prime-minister-ariel-henry-resigns},
	abstract = {Haitian Prime Minister Ariel Henry has resigned as head of the Caribbean nation, coming amid a surge in gang violence. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_hong_nodate,
	title = {Hong {Kong} {China} {Legislation} {Dissent} {\textbar} {Rev}},
	url = {https://www.rev.com/blog/transcripts/hong-kong-passes-new-security-law-to-crack-down-on-dissent},
	abstract = {A new law in Hong Kong will crack down on all forms of dissent and could affect the freedoms of people and foreign businesses. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_baltimore_nodate,
	title = {Baltimore {Bridge} {Collapse} {Update} {Francis} {Scott} {Key} {\textbar} {Rev}},
	url = {https://www.rev.com/blog/transcripts/officials-give-briefing-on-baltimore-bridge-collapse},
	abstract = {Officials give update on Baltimore bridge collapse. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_remembering_nodate,
	title = {Remembering {Louis} {Gossett} {Jr}.},
	url = {https://www.rev.com/blog/transcripts/remembering-louis-gossett-jr},
	abstract = {A look back at the illustrious career and life of the Academy Award winner who died at age 87. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_iran_nodate,
	title = {Iran {Consulate} {Syria} {Israel} {Airstrike} {\textbar} {Rev}},
	url = {https://www.rev.com/blog/transcripts/israeli-attack-on-iran-consulate-in-syria},
	abstract = {A top commander in Iran’s Revolutionary Guard has been killed in an airstrike on the country’s consulate building in Damascus, Syria. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{allen_bankman-fried_nodate,
	title = {Bankman-{Fried} {Sentenced} to 25 {Years} in {Prison} for {Fraud}},
	url = {https://www.rev.com/blog/transcripts/bankman-fried-sentenced-to-25-years-in-prison-for-fraud},
	abstract = {Sam Bankman-Fried was sentenced to 25 years in prison for one of the biggest financial crimes in U.S. history. Read the transcript here.},
	language = {en-US},
	urldate = {2024-04-30},
	journal = {Rev Blog},
	author = {Allen, Hugh},
}

@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06825 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{radford_robust_2022,
	title = {Robust {Speech} {Recognition} via {Large}-{Scale} {Weak} {Supervision}},
	url = {http://arxiv.org/abs/2212.04356},
	doi = {10.48550/arXiv.2212.04356},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04356 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{stinkyelias_source_2024,
	title = {Source build failing due to missing packages · {Issue} \#4169 · ros-planning/navigation2},
	url = {https://github.com/ros-planning/navigation2/issues/4169},
	urldate = {2024-04-24},
	journal = {GitHub},
	author = {stinkyElias},
	month = jul,
	year = {2024},
}

@article{coles_forward-chaining_2021,
	title = {Forward-{Chaining} {Partial}-{Order} {Planning}},
	volume = {20},
	issn = {2334-0843, 2334-0835},
	url = {https://ojs.aaai.org/index.php/ICAPS/article/view/13403},
	doi = {10.1609/icaps.v20i1.13403},
	abstract = {Over the last few years there has been a revival of interest in the idea of least-commitment planning with a number of researchers returning to the partial-order planning approaches of UCPOP and VHPOP. In this paper we explore the potential of a forward-chaining state-based search strategy to support partial-order planning in the solution of temporal-numeric problems. Our planner, POPF, is built on the foundations of grounded forward search, in combination with linear programming to handle continuous linear numeric change. To achieve a partial ordering we delay commitment to ordering decisions, timestamps and the values of numeric parameters, managing sets of constraints as actions are started and ended. In the context of a partially ordered collection of actions, constructing the linear program is complicated and we propose an efﬁcient method for achieving this. Our late-commitment approach achieves ﬂexibility, while beneﬁting from the informative search control of forward planning, and allows temporal and metric decisions to be made — as is most efﬁcient —by the LP solver rather than by the discrete reasoning of the planner. We compare POPF with the approach of constructing a sequenced plan and then lifting a partial order from it, showing that our approach can offer improvements in terms of makespan, and time to ﬁnd a solution, in several benchmark domains.},
	language = {en},
	urldate = {2024-04-24},
	journal = {Proceedings of the International Conference on Automated Planning and Scheduling},
	author = {Coles, Amanda and Coles, Andrew and Fox, Maria and Long, Derek},
	month = may,
	year = {2021},
	pages = {42--49},
}

@article{fox_pddl21_2003,
	title = {{PDDL2}.1: {An} {Extension} to {PDDL} for {Expressing} {Temporal} {Planning} {Domains}},
	volume = {20},
	issn = {1076-9757},
	shorttitle = {{PDDL2}.1},
	url = {http://arxiv.org/abs/1106.4561},
	doi = {10.1613/jair.1129},
	abstract = {In recent years research in the planning community has moved increasingly toward s application of planners to realistic problems involving both time and many typ es of resources. For example, interest in planning demonstrated by the space res earch community has inspired work in observation scheduling, planetary rover ex ploration and spacecraft control domains. Other temporal and resource-intensive domains including logistics planning, plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application. The International Planning Competitions have acted as an important motivating fo rce behind the progress that has been made in planning since 1998. The third com petition (held in 2002) set the planning community the challenge of handling tim e and numeric resources. This necessitated the development of a modelling langua ge capable of expressing temporal and numeric properties of planning domains. In this paper we describe the language, PDDL2.1, that was used in the competition. We describe the syntax of the language, its formal semantics and the validation of concurrent plans. We observe that PDDL2.1 has considerable modelling power --- exceeding the capabilities of current planning technology --- and presents a number of important challenges to the research community.},
	urldate = {2024-04-24},
	journal = {Journal of Artificial Intelligence Research},
	author = {Fox, M. and Long, D.},
	month = dec,
	year = {2003},
	note = {arXiv:1106.4561 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {61--124},
}

@misc{srivastav_vaibhavs10insanely-fast-whisper_2024,
	title = {Vaibhavs10/insanely-fast-whisper},
	copyright = {Apache-2.0},
	shorttitle = {Insanely {Fast} {Whisper}},
	url = {https://github.com/Vaibhavs10/insanely-fast-whisper},
	urldate = {2024-04-24},
	author = {Srivastav, Vaibhav},
	month = apr,
	year = {2024},
	note = {original-date: 2023-10-10T15:17:49Z},
}

@misc{martin_optimized_2021,
	title = {Optimized {Execution} of {PDDL} {Plans} using {Behavior} {Trees}},
	url = {http://arxiv.org/abs/2101.01964},
	doi = {10.48550/arXiv.2101.01964},
	abstract = {Robots need task planning to sequence and execute actions toward achieving their goals. On the other hand, Behavior Trees provide a mathematical model for specifying plan execution in an intrinsically composable, reactive, and robust way. PDDL (Planning Domain Definition Language) has become the standard description language for most planners. In this paper, we present a novel algorithm to systematically create behavior trees from PDDL plans to execute them. This approach uses the execution graph of the plan to generate a behavior tree. The most remarkable contribution of this approach is the algorithm to build a Behavior Tree that optimizes its execution by paralyzing actions, applicable to any plan, taking into account the actions' causal relationships. We demonstrate the improvement in the execution of plans in mobile robots using the ROS2 Planning System framework.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Martín, Francisco and Morelli, Matteo and Espinoza, Huascar and Lera, Francisco J. R. and Matellán, Vicente},
	month = jan,
	year = {2021},
	note = {arXiv:2101.01964 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{guan_leveraging_2023,
	title = {Leveraging {Pre}-trained {Large} {Language} {Models} to {Construct} and {Utilize} {World} {Models} for {Model}-based {Task} {Planning}},
	url = {http://arxiv.org/abs/2305.14909},
	doi = {10.48550/arXiv.2305.14909},
	abstract = {There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Guan, Lin and Valmeekam, Karthik and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = nov,
	year = {2023},
	note = {arXiv:2305.14909 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{martin_plansys2_2021,
	title = {{PlanSys2}: {A} {Planning} {System} {Framework} for {ROS2}},
	shorttitle = {{PlanSys2}},
	url = {http://arxiv.org/abs/2107.00376},
	doi = {10.48550/arXiv.2107.00376},
	abstract = {Autonomous robots need to plan the tasks they carry out to fulfill their missions. The missions' increasing complexity does not let human designers anticipate all the possible situations, so traditional control systems based on state machines are not enough. This paper contains a description of the ROS2 Planning System (PlanSys2 in short), a framework for symbolic planning that incorporates novel approaches for execution on robots working in demanding environments. PlanSys2 aims to be the reference task planning framework in ROS2, the latest version of the \{{\textbackslash}em de facto\} standard in robotics software development. Among its main features, it can be highlighted the optimized execution, based on Behavior Trees, of plans through a new actions auction protocol and its multi-robot planning capabilities. It already has a small but growing community of users and developers, and this document is a summary of the design and capabilities of this project.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Martín, Francisco and Ginés, Jonatan and Matellán, Vicente and Rodríguez, Francisco J.},
	month = jul,
	year = {2021},
	note = {arXiv:2107.00376 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@article{ghallab_pddl_1998,
	title = {{PDDL} - {The} {Planning} {Domain} {Definition} {Language}},
	abstract = {This manual describes the syntax of PDDL, the Planning Domain Definition Language, the problem-specification language for the AIPS-98 planning competition. The language has roughly the the expressiveness of Pednault's ADL [10] for propositions, and roughly the expressiveness of UMCP [6] for actions. Our hope is to encourage empirical evaluation of planner performance, and development of standard sets of problems all in comparable notations. 1 Introduction This manual describes the syntax, and, less formally, the semantics, of the Planning Domain Definition Language (PDDL). The language supports the following syntactic features: ffl Basic STRIPS-style actions ffl Conditional effects ffl Universal quantification over dynamic universes (i.e., object creation and destruction), ffl Domain axioms over stratified theories, ffl Specification of safety constraints. ffl Specification of hierarchical actions composed of subactions and subgoals. ffl Management of multiple problems in mul...},
	author = {Ghallab, Malik and Knoblock, Craig and Wilkins, David and Barrett, Anthony and Christianson, Dave and Friedman, Marc and Kwok, Chung and Golden, Keith and Penberthy, Scott and Smith, David and Sun, Ying and Weld, Daniel},
	month = aug,
	year = {1998},
}

@book{colledanchise_behavior_2018,
	title = {Behavior {Trees} in {Robotics} and {AI}: {An} {Introduction}},
	shorttitle = {Behavior {Trees} in {Robotics} and {AI}},
	url = {http://arxiv.org/abs/1709.00084},
	abstract = {A Behavior Tree (BT) is a way to structure the switching between different tasks in an autonomous agent, such as a robot or a virtual entity in a computer game. BTs are a very efficient way of creating complex systems that are both modular and reactive. These properties are crucial in many applications, which has led to the spread of BT from computer game programming to many branches of AI and Robotics. In this book, we will first give an introduction to BTs, then we describe how BTs relate to, and in many cases generalize, earlier switching structures. These ideas are then used as a foundation for a set of efficient and easy to use design principles. Properties such as safety, robustness, and efficiency are important for an autonomous system, and we describe a set of tools for formally analyzing these using a state space description of BTs. With the new analysis tools, we can formalize the descriptions of how BTs generalize earlier approaches. We also show the use of BTs in automated planning and machine learning. Finally, we describe an extended set of tools to capture the behavior of Stochastic BTs, where the outcomes of actions are described by probabilities. These tools enable the computation of both success probabilities and time to completion.},
	urldate = {2024-01-31},
	author = {Colledanchise, Michele and Ögren, Petter},
	month = jul,
	year = {2018},
	doi = {10.1201/9780429489105},
	note = {arXiv:1709.00084 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{alaboud_getting_2024,
	title = {Getting {Started} with {PDDL}},
	url = {https://fareskalaboud.github.io/LearnPDDL/},
	abstract = {[WORK IN PROGRESS] A guide to learning, implementing and using PDDL. (Updated Weekly)},
	language = {en-US},
	urldate = {2024-01-30},
	journal = {LearnPDDL},
	author = {Alaboud, Fares and Coles, Andrew},
	year = {2024},
}

@misc{auryn_robotics_about_2024,
	title = {About {\textbar} {BehaviorTree}.{CPP}},
	url = {https://www.behaviortree.dev/docs/intro},
	abstract = {About this library},
	language = {en-US},
	urldate = {2024-01-30},
	author = {Auryn Robotics},
	year = {2024},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	doi = {10.48550/arXiv.2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{micikevicius_fp8_2022,
	title = {{FP8} {Formats} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2209.05433},
	abstract = {FP8 is a natural progression for accelerating deep learning training inference beyond the 16-bit formats common in modern processors. In this paper we propose an 8-bit floating point (FP8) binary interchange format consisting of two encodings - E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). While E5M2 follows IEEE 754 conventions for representatio of special values, E4M3's dynamic range is extended by not representing infinities and having only one mantissa bit-pattern for NaNs. We demonstrate the efficacy of the FP8 format on a variety of image and language tasks, effectively matching the result quality achieved by 16-bit training sessions. Our study covers the main modern neural network architectures - CNNs, RNNs, and Transformer-based models, leaving all the hyperparameters unchanged from the 16-bit baseline training sessions. Our training experiments include large, up to 175B parameter, language models. We also examine FP8 post-training-quantization of language models trained using 16-bit formats that resisted fixed point int8 quantization.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and Mellempudi, Naveen and Oberman, Stuart and Shoeybi, Mohammad and Siu, Michael and Wu, Hao},
	month = sep,
	year = {2022},
	note = {arXiv:2209.05433 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{belkada_making_2023,
	title = {Making {LLMs} even more accessible with bitsandbytes, 4-bit quantization and {QLoRA}},
	url = {https://huggingface.co/blog/4bit-transformers-bitsandbytes},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-01-24},
	journal = {Hugging Face},
	author = {Belkada, Younes and Dettmers, Tim and Pagnoni, Artidoro and Gugger, Sylvain and Mangrulkar, Sourab},
	month = may,
	year = {2023},
}

@misc{dettmers_llmint8_2022,
	title = {{LLM}.int8(): 8-bit {Matrix} {Multiplication} for {Transformers} at {Scale}},
	shorttitle = {{LLM}.int8()},
	url = {http://arxiv.org/abs/2208.07339},
	doi = {10.48550/arXiv.2208.07339},
	abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
	month = nov,
	year = {2022},
	note = {arXiv:2208.07339 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{belkada_gentle_2022,
	title = {A {Gentle} {Introduction} to 8-bit {Matrix} {Multiplication} for transformers at scale using transformers, accelerate and bitsandbytes},
	url = {https://huggingface.co/blog/hf-bitsandbytes-integration},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-01-24},
	journal = {Hugging Face},
	author = {Belkada, Younes and Dettmers, Tim},
	month = aug,
	year = {2022},
}

@misc{silver_generalized_2023,
	title = {Generalized {Planning} in {PDDL} {Domains} with {Pretrained} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.11014},
	doi = {10.48550/arXiv.2305.11014},
	abstract = {Recent work has considered whether large language models (LLMs) can function as planners: given a task, generate a plan. We investigate whether LLMs can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain. In particular, we consider PDDL domains and use GPT-4 to synthesize Python programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program; and (2) automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback. We evaluate this approach in seven PDDL domains and compare it to four ablations and four baselines. Overall, we find that GPT-4 is a surprisingly powerful generalized planner. We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two training tasks are often sufficient for strong generalization.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Silver, Tom and Dan, Soham and Srinivas, Kavitha and Tenenbaum, Joshua B. and Kaelbling, Leslie Pack and Katz, Michael},
	month = dec,
	year = {2023},
	note = {arXiv:2305.11014 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{hugging_face_perplexity_2024,
	title = {Perplexity of fixed-length models},
	url = {https://huggingface.co/docs/transformers/perplexity},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-01-25},
	author = {Hugging Face},
	month = jan,
	year = {2024},
}
